### [● Open Interpreter](https://github.com/OpenInterpreter/open-interpreter)

> Handle: `opint`
> URL: -

<p align="center">
    <a href="https://discord.gg/Hvz9Axh84z">
        <img alt="Discord" src="https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white"/></a>
    <a href="https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/README_JA.md"><img src="https://img.shields.io/badge/ドキュメント-日本語-white.svg" alt="JA doc"/></a>
    <a href="https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/README_ZH.md"><img src="https://img.shields.io/badge/文档-中文版-white.svg" alt="ZH doc"/></a>
    <a href="https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/README_ES.md"> <img src="https://img.shields.io/badge/Español-white.svg" alt="ES doc"/></a>
    <a href="https://github.com/OpenInterpreter/open-interpreter/blob/main/docs/README_IN.md"><img src="https://img.shields.io/badge/Hindi-white.svg" alt="IN doc"/></a>
    <a href="https://github.com/OpenInterpreter/open-interpreter/blob/main/LICENSE"><img src="https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat" alt="License"/></a>
</p>

#### Overview

Open Interpreter lets LLMs run code (Python, Javascript, Shell, and more) locally. You can chat with Open Interpreter through a ChatGPT-like interface in your terminal.

#### Starting

> Note that Harbor uses shortened `opint` service handle. For the CLI, you are free to use either official `interpreter` or `opint` alias.

Harbor will allow you running `interpreter` as if it was installed on your local machine. A big disclaimer is that Harbor only allows for the features of `interpreter` that are compatible with Docker runtime. [Official Docker Integration](https://docs.openinterpreter.com/integrations/docker) outlines those nicely.

We'll refer to the service as `opint` from now on.

```bash
# Pre-build the image for convenience
harbor build opint

# opint is only configured to run
# alongside the LLM backend service (ollama, litellm, mistral.rs),
# check that at least one of them is running, otherwise
# you'll see connection errors
harbor ps

# See official CLI help
harbor opint --help
```

#### Configuration

##### Profiles

See [official profiles doc](https://docs.openinterpreter.com/guides/profiles)

```bash
# See where profiles are located on the host
# Modify the profiles as needed
harbor opint profiles

# Ensure that specific model is unset before
# setting the profile
harbor opint model ""
harbor opint args --profile <name>

# [Alternative] Set via opint.cmd config
# Note, it resets .model and .args
harbor opint cmd --profile <name>
```

##### Ollama

`opin` is pre-configured to run with `ollama` when it is also running.

```bash
# 0. Check your current default services
# ollama should be one of them
# See ollama models you have available
harbor defaults
harbor ollama models

# 1.1 You want to choose as big of a model
# as you can afford for the best experience
harbor opint model codestral

# Execute in the target folder
harbor opint
```

##### vLLM

```bash
# [Optional] If running __multiple__ backends
# at a time, you'll need to point opint to one of them
harbor opint backend vllm

# Set opint to use one of the models from
# /v1/models endpoint of the backend
harbor opint model google/gemma-2-2b-it

# Execute in the target folder
harbor opint
```

##### Other backends

To check if a backend is integrated with `opint` - lookup `compose.x.opint.<backend>.yml` file in the Harbor workspace.

The setup is identical to `vllm`:
- if running multiple backends, ensure that `opint` is pointed to one of them
- ensure that `opint` is configured to use one of the models from the backend's OpenAI API
