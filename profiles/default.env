# ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░
# ▒█░▒█ ░█▀▀█ ▒█▀▀█ ▒█▀▀█ ▒█▀▀▀█ ▒█▀▀█
# ▒█▀▀█ ▒█▄▄█ ▒█▄▄▀ ▒█▀▀▄ ▒█░░▒█ ▒█▄▄▀
# ▒█░▒█ ▒█░▒█ ▒█░▒█ ▒█▄▄█ ▒█▄▄▄█ ▒█░▒█
# ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░

# Harbor Configuration.
# ---------------------
# This section contains environment variables that are
# used in Harbor's compose files, configs and can be modified via harbor CLI.
#
# | Using CLI for config management:
# |
# | ```bash
# | harbor config get hf.cache
# | harbor config set hf.cache ~/.cache/huggingface
# | ```
# |
# | See more at https://github.com/av/harbor/wiki/Harbor-CLI-Reference

# Abstract/shared
# ---------------------

HARBOR_HF_CACHE="~/.cache/huggingface"
HARBOR_HF_TOKEN=""

HARBOR_LLAMACPP_CACHE="~/.cache/llama.cpp"
HARBOR_OLLAMA_CACHE="~/.ollama"
HARBOR_VLLM_CACHE="~/.cache/vllm"
HARBOR_TXTAI_CACHE="~/.cache/txtai"
HARBOR_NEXA_CACHE="~/.cache/nexa"

# These could be used by specific services,
# in which case they can be set in a centralised
# location like this.
HARBOR_ANYSCALE_KEY=""
HARBOR_APIPIE_KEY=""
HARBOR_COHERE_KEY=""
HARBOR_FIREWORKS_API_KEY=""
HARBOR_GROQ_KEY=""
HARBOR_MISTRAL_KEY=""
HARBOR_OPENROUTER_KEY=""
HARBOR_PERPLEXITY_KEY=""
HARBOR_SHUTTLEAI_KEY=""
HARBOR_TOGETHERAI_KEY=""
HARBOR_ANTHROPIC_KEY=""
HARBOR_BINGAI_TOKEN=""
HARBOR_GOOGLE_KEY=""
HARBOR_ASSISTANTS_KEY=""
HARBOR_CIVITAI_TOKEN=""

HARBOR_UI_MAIN="webui"
HARBOR_UI_AUTOOPEN=false
HARBOR_SERVICES_DEFAULT="ollama;webui"
HARBOR_SERVICES_TUNNELS=""
HARBOR_CONTAINER_PREFIX="harbor"
HARBOR_CLI_NAME="harbor"
HARBOR_CLI_SHORT="h"
HARBOR_CLI_PATH="~/.local/bin"
HARBOR_LOG_LEVEL="INFO"
HARBOR_HISTORY_SIZE=100
HARBOR_HISTORY_FILE="./.history"
# Aliases for "harbor run", manage with "harbor alias"
HARBOR_ALIASES=""
# When set to false, Harbor won't try to infer
# capabilities of the host machine, like "nvidia" or "cdi"
HARBOR_CAPABILITIES_AUTODETECT=true
# Colon-separated list of capabilities to enforce
HARBOR_CAPABILITIES_DEFAULT=""
HARBOR_LEGACY_CLI=false
HARBOR_TOOLS=""

# Written by Harbor CLI
# to be used in the containers - modifying
# won't do anything
HARBOR_USER_ID=""
HARBOR_GROUP_ID=""
HARBOR_HOME_VOLUME=""

# OpenAI
# ---------------------
# In the Context of Harbor, it means OpenAI API-compatible
# services, such as Ollama, Llama.cpp, LiteLLM, etc.

HARBOR_OPENAI_URLS=""
HARBOR_OPENAI_KEYS=""

# This variable is derived as a first item in the list above
HARBOR_OPENAI_KEY=""
HARBOR_OPENAI_URL=""

# webui
HARBOR_WEBUI_HOST_PORT=33801
# Persistent secret - user stays logged into
# webui between restarts
HARBOR_WEBUI_SECRET="h@rb0r"
HARBOR_WEBUI_NAME="Harbor"
HARBOR_WEBUI_LOG_LEVEL="DEBUG"
HARBOR_WEBUI_VERSION="main"
HARBOR_WEBUI_IMAGE="ghcr.io/open-webui/open-webui"

# Open WebUI Pipelines
HARBOR_PIPELINES_HOST_PORT=34211
HARBOR_PIPELINES_API_KEY="sk-harbor-0p3n-w3bu!"
HARBOR_PIPELINES_URLS=""
HARBOR_PIPELINES_DIR="/app/pipelines"
HARBOR_PIPELINES_RESET_DIR=false
HARBOR_PIPELINES_REQUIREMENTS_PATH=""
HARBOR_PIPELINES_VERSION="main"

# llamacpp
HARBOR_LLAMACPP_HOST_PORT=33831
HARBOR_LLAMACPP_GGUF=""
HARBOR_LLAMACPP_MODEL="https://huggingface.co/microsoft/Phi-3.5-mini-instruct-gguf/blob/main/Phi-3-mini-4k-instruct-q4.gguf"
HARBOR_LLAMACPP_MODEL_SPECIFIER="--hf-repo microsoft/Phi-3.5-mini-instruct-gguf --hf-file Phi-3-mini-4k-instruct-q4.gguf"
HARBOR_LLAMACPP_EXTRA_ARGS="-ngl 32"

# ollama
HARBOR_OLLAMA_HOST_PORT=33821
HARBOR_OLLAMA_VERSION="latest"
HARBOR_OLLAMA_INTERNAL_URL="http://ollama:11434"
HARBOR_OLLAMA_DEFAULT_MODELS="mxbai-embed-large:latest"
HARBOR_OLLAMA_CONTEXT_LENGTH=4096

# litellm
HARBOR_LITELLM_HOST_PORT=33841
HARBOR_LITELLM_MASTER_KEY="sk-litellm"
HARBOR_LITELLM_DB_HOST_PORT=33842
HARBOR_LITELLM_UI_USERNAME="admin"
HARBOR_LITELLM_UI_PASSWORD="admin"

# lmdeploy
HARBOR_LMDEPLOY_HOST_PORT=33831

# searxng
HARBOR_SEARXNG_HOST_PORT=33811
HARBOR_SEARXNG_IMAGE="searxng/searxng"
HARBOR_SEARXNG_VERSION="latest"
HARBOR_SEARXNG_INTERNAL_URL="http://searxng:8080"
HARBOR_SEARXNG_WORKSPACE="./searxng"

# tgi (text-generation-inference)
HARBOR_TGI_HOST_PORT=33851
HARBOR_TGI_MODEL="google/gemma-2-2b-it"
HARBOR_TGI_QUANT=""
HARBOR_TGI_REVISION=""
HARBOR_TGI_EXTRA_ARGS="--max-concurrent-requests 16"
HARBOR_TGI_MODEL_SPECIFIER="--model-id google/gemma-2-2b-it"

# tts (openedai-sppech)
HARBOR_TTS_HOST_PORT=33861
HARBOR_TTS_HOME="voices"
HARBOR_TTS_VOICES_FOLDER="./tts/voices"
HARBOR_TTS_CONFIG_FOLDER="./tts/config"

# hollama
HARBOR_HOLLAMA_HOST_PORT=33871

# LangFuse
HARBOR_LANGFUSE_HOST_PORT=33881
HARBOR_LANGFUSE_NEXTAUTH_SECRET="langfuse"
HARBOR_LANGFUSE_SALT="salt"
HARBOR_LANGFUSE_DB_HOST_PORT=33882
# These should be set when configuring
# new project in the service
HARBOR_LANGFUSE_PUBLIC_KEY=""
HARBOR_LANGFUSE_SECRET_KEY=""
HARBOR_LANGFUSE_WORKSPACE="./langfuse/data"

# LibreChat
HARBOR_LIBRECHAT_HOST_PORT=33891
HARBOR_LIBRECHAT_RAG_HOST_PORT=33892

# BionicGPT
HARBOR_BIONICGPT_HOST_PORT=33901

# vLLM
HARBOR_VLLM_HOST_PORT=33911
HARBOR_VLLM_IMAGE="vllm/vllm-openai"
HARBOR_VLLM_VERSION="v0.8.5"
HARBOR_VLLM_MODEL="microsoft/Phi-3.5-mini-instruct"
HARBOR_VLLM_EXTRA_ARGS=""
HARBOR_VLLM_ATTENTION_BACKEND="FLASH_ATTN"
HARBOR_VLLM_MODEL_SPECIFIER="--model microsoft/Phi-3.5-mini-instruct"

# Aphrodite
HARBOR_APHRODITE_HOST_PORT=33921
HARBOR_APHRODITE_VERSION="latest"
HARBOR_APHRODITE_EXTRA_ARGS=""
HARBOR_APHRODITE_MODEL="neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit"

# TabbyAPI
HARBOR_TABBYAPI_HOST_PORT=33931
HARBOR_TABBYAPI_ADMIN_KEY="adk-tabbyapi"
HARBOR_TABBYAPI_API_KEY="apk-tabbyapi"
HARBOR_TABBYAPI_MODEL="Annuvin/gemma-2-2b-it-abliterated-4.0bpw-exl2"
HARBOR_TABBYAPI_MODEL_SPECIFIER="Annuvin_gemma-2-2b-it-abliterated-4.0bpw-exl2"
HARBOR_TABBYAPI_EXTRA_ARGS=""

# Parllama
HARBOR_PARLLAMA_CACHE="~/.parllama"

# Plandex
HARBOR_PLANDEX_HOST_PORT=33941
HARBOR_PLANDEX_DB_HOST_PORT=33942
HARBOR_PLANDEX_HOME="~/.plandex-home"

# Mistral.rs
HARBOR_MISTRALRS_HOST_PORT=33951
HARBOR_MISTRALRS_VERSION="0.3"
HARBOR_MISTRALRS_MODEL_TYPE="plain"
HARBOR_MISTRALRS_MODEL="microsoft/Phi-3.5-mini-instruct"
HARBOR_MISTRALRS_MODEL_ARCH="phi3"
HARBOR_MISTRALRS_MODEL_ISQ=""
HARBOR_MISTRALRS_MODEL_SPECIFIER="plain -m microsoft/Phi-3.5-mini-instruct -a phi3"
HARBOR_MISTRALRS_EXTRA_ARGS=""

# Open Interpreter
HARBOR_OPINT_CONFIG_PATH="~/.config/open-interpreter"
HARBOR_OPINT_EXTRA_ARGS=""
HARBOR_OPINT_MODEL="llama3.1"
HARBOR_OPINT_CMD="--model llama3.1"
HARBOR_OPINT_BACKEND=""

# cmdh
HARBOR_CMDH_MODEL="llama3.1"
HARBOR_CMDH_LLM_HOST="ollama"
HARBOR_CMDH_LLM_KEY=""
HARBOR_CMDH_LLM_URL=""

# Dify
HARBOR_DIFY_HOST_PORT=33961
HARBOR_DIFY_DB_HOST_PORT=33962
HARBOR_DIFY_D2O_HOST_PORT=33963
HARBOR_DIFY_VERSION="0.6.16"
HARBOR_DIFY_SANDBOX_VERSION="0.2.1"
HARBOR_DIFY_WEAVIATE_VERSION="1.19.0"
HARBOR_DIFY_VOLUMES="./dify/volumes"
HARBOR_DIFY_BOT_TYPE="Chat"
HARBOR_DIFY_OPENAI_WORKFLOW=""

# Fabric
HARBOR_FABRIC_CONFIG_PATH="~/.config/fabric"
HARBOR_FABRIC_MODEL="llama3.1:8b"

# Parler
HARBOR_PARLER_HOST_PORT=33971
HARBOR_PARLER_MODEL="parler-tts/parler-tts-mini-v1"
HARBOR_PARLER_VOICE="Alisa speaks in calm but steady voice. A very clear audio."

# AirLLM
HARBOR_AIRLLM_HOST_PORT=33981
HARBOR_AIRLLM_MODEL="meta-llama/Meta-Llama-3.1-8B-Instruct"
HARBOR_AIRLLM_CTX_LEN=128
HARBOR_AIRLLM_COMPRESSION="4bit"

# txtai
HARBOR_TXTAI_RAG_HOST_PORT=33991
HARBOR_TXTAI_RAG_MODEL="llama3.1:8b-instruct-q4_K_M"
HARBOR_TXTAI_RAG_EMBEDDINGS="neuml/txtai-wikipedia-slim"

# TextGrad
HARBOR_TEXTGRAD_HOST_PORT=34001

# Aider
HARBOR_AIDER_HOST_PORT=34011
HARBOR_AIDER_MODEL="llama3.1:8b-instruct-q6_K"

# HuggingFace ChatUI
HARBOR_CHATUI_HOST_PORT=34021
HARBOR_CHATUI_VERSION="latest"
HARBOR_CHATUI_OLLAMA_MODEL="llama3.1:8b"
HARBOR_CHATUI_LITELLM_MODEL="llama3.1:8b"

# ComfyUI
HARBOR_COMFYUI_HOST_PORT=34031
HARBOR_COMFYUI_PORTAL_HOST_PORT=34032
HARBOR_COMFYUI_SYNCTHING_HOST_PORT=34033
HARBOR_COMFYUI_VERSION="latest-cuda"
HARBOR_COMFYUI_AUTH='false'
HARBOR_COMFYUI_USER="harbor"
HARBOR_COMFYUI_PASSWORD="sk-comfyui"
HARBOR_COMFYUI_ARGS=""
HARBOR_COMFYUI_WORKSPACE="./comfyui/workspace"
HARBOR_COMFYUI_PROVISIONING="https://raw.githubusercontent.com/av/harbor/main/comfyui/provisioning.sh"

# Perplexica
HARBOR_PERPLEXICA_HOST_PORT=34041
HARBOR_PERPLEXICA_BACKEND_HOST_PORT=34042

# Aichat
HARBOR_AICHAT_HOST_PORT=34051
HARBOR_AICHAT_MODEL="llama3.1:8b"
HARBOR_AICHAT_CONFIG_PATH="~/.config/aichat"

# AutoGPT
HARBOR_AUTOGPT_HOST_PORT=34061
HARBOR_AUTOGPT_MODEL="llama3.1:8b"

# LobeChat
HARBOR_LOBECHAT_HOST_PORT=34071
HARBOR_LOBECHAT_VERSION="latest"

# Omnichain
HARBOR_OMNICHAIN_HOST_PORT=34081
HARBOR_OMNICHAIN_API_HOST_PORT=34082
HARBOR_OMNICHAIN_WORKSPACE="./omnichain"

# Bench
HARBOR_BENCH_PARALLEL=1
HARBOR_BENCH_DEBUG=false
HARBOR_BENCH_MODEL="llama3.1:8b"
HARBOR_BENCH_API="http://ollama:11434"
HARBOR_BENCH_API_KEY=""
HARBOR_BENCH_VARIANTS=""
HARBOR_BENCH_JUDGE="mistral-nemo:12b-instruct-2407-q8_0"
HARBOR_BENCH_JUDGE_API="http://ollama:11434"
HARBOR_BENCH_JUDGE_API_KEY=""
HARBOR_BENCH_JUDGE_PROMPT="default"
HARBOR_BENCH_RESULTS="./bench/results"
HARBOR_BENCH_TASKS="./bench/defaultTasks.yml"

# lm_eval
HARBOR_LMEVAL_TYPE="local-completions"
HARBOR_LMEVAL_RESULTS="./lmeval/results"
HARBOR_LMEVAL_CACHE="./lmeval/cache"
HARBOR_LMEVAL_EXTRA_ARGS=""
HARBOR_LMEVAL_MODEL_SPECIFIER=""
HARBOR_LMEVAL_MODEL_ARGS=""

# SGLang
HARBOR_SGLANG_HOST_PORT=34091
HARBOR_SGLANG_VERSION="latest"
HARBOR_SGLANG_MODEL="google/gemma-2-2b-it"
HARBOR_SGLANG_EXTRA_ARGS=""

# Jupyter
HARBOR_JUPYTER_HOST_PORT=34101
HARBOR_JUPYTER_IMAGE="pytorch/pytorch:2.3.0-cuda12.1-cudnn8-runtime"
HARBOR_JUPYTER_WORKSPACE="./jupyter/workspace"
HARBOR_JUPYTER_EXTRA_DEPS=""

# ol1
HARBOR_OL1_HOST_PORT=34111
HARBOR_OL1_MODEL="llama3.1:8b"
HARBOR_OL1_ARGS="temperature=0.2"

# ktransformers
HARBOR_KTRANSFORMERS_HOST_PORT=34121
HARBOR_KTRANSFORMERS_VERSION="0.1.4"
HARBOR_KTRANSFORMERS_IMAGE="pytorch/pytorch:2.3.1-cuda12.1-cudnn8-devel"
HARBOR_KTRANSFORMERS_MODEL=""
HARBOR_KTRANSFORMERS_GGUF=""
HARBOR_KTRANSFORMERS_EXTRA_ARGS=""

# Boost
HARBOR_BOOST_HOST_PORT=34131
HARBOR_BOOST_OPENAI_URLS=""
HARBOR_BOOST_OPENAI_KEYS=""
HARBOR_BOOST_MODULES="klmbr;rcn;g1;mcts;eli5;supersummer"
HARBOR_BOOST_MODULE_FOLDERS="modules;custom_modules"
HARBOR_BOOST_INTERMEDIATE_OUTPUT=true
HARBOR_BOOST_STATUS_STYLE="md:codeblock"
HARBOR_BOOST_BASE_MODELS="false"
HARBOR_BOOST_MODEL_FILTER=""
HARBOR_BOOST_API_KEY="sk-boost"
HARBOR_BOOST_API_KEYS=""
# Boost - klmbr
HARBOR_BOOST_KLMBR_PERCENTAGE=35
HARBOR_BOOST_KLMBR_MODS="all"
HARBOR_BOOST_KLMBR_STRAT="match"
HARBOR_BOOST_KLMBR_STRAT_PARAMS="role=user"
# Boost - rcn
HARBOR_BOOST_RCN_STRAT="match"
HARBOR_BOOST_RCN_STRAT_PARAMS="role=user,index=-1"
# Boost - g1
HARBOR_BOOST_G1_STRAT="match"
HARBOR_BOOST_G1_STRAT_PARAMS="role=user,index=-1"
HARBOR_BOOST_G1_MAX_STEPS=15
# Boost - mcts
HARBOR_BOOST_MCTS_STRAT="match"
HARBOR_BOOST_MCTS_STRAT_PARAMS="role=user,index=-1"
HARBOR_BOOST_MCTS_MAX_SIMULATIONS=2
HARBOR_BOOST_MCTS_MAX_ITERATIONS=2
HARBOR_BOOST_MCTS_THOUGHTS=2
HARBOR_BOOST_MCTS_EXPLORATION_CONSTANT=1.414
# Boost - eli5
HARBOR_BOOST_ELI5_STRAT="match"
HARBOR_BOOST_ELI5_STRAT_PARAMS="role=user,index=-1"
# Boost - supersummer
HARBOR_BOOST_SUPERSUMMER_STRAT="match"
HARBOR_BOOST_SUPERSUMMER_STRAT_PARAMS="role=user,index=-1"
HARBOR_BOOST_SUPERSUMMER_NUM_QUESTIONS=5
HARBOR_BOOST_SUPERSUMMER_LENGTH="few paragraphs"
# Boost - r0
HARBOR_BOOST_R0_THOUGHTS=5

# OpenHands
HARBOR_OPENHANDS_HOST_PORT=34141
HARBOR_OPENHANDS_VERSION="latest"

# STT aka faster-whisper-server
HARBOR_STT_HOST_PORT=34151
HARBOR_STT_VERSION="latest"
HARBOR_STT_MODEL="Systran/faster-distil-whisper-large-v3"

# LitLytics
HARBOR_LITLYTICS_HOST_PORT=34161
HARBOR_LITLYTICS_VERSION="latest"

# AnythingLLM
HARBOR_ANYTHINGLLM_HOST_PORT=34171
HARBOR_ANYTHINGLLM_IMAGE="mintplexlabs/anythingllm"
HARBOR_ANYTHINGLLM_VERSION="latest"
HARBOR_ANYTHINGLLM_JWT_SECRET="sk-anythingllm-jwt"

# Nexa AI SDK
HARBOR_NEXA_HOST_PORT=34181
HARBOR_NEXA_MODEL="llama3.2"

# n8n
HARBOR_N8N_HOST_PORT=34191
HARBOR_N8N_PG_HOST_PORT=34192
HARBOR_N8N_WORKSPACE="./n8n"
HARBOR_N8N_IMAGE="n8nio/n8n"
HARBOR_N8N_VERSION="latest"
HARBOR_N8N_PG_IMAGE="postgres"
HARBOR_N8N_PG_VERSION="16-alpine"
HARBOR_N8N_PG_USER="harbor"
HARBOR_N8N_PG_PASSWORD="sk-harbor"
HARBOR_N8N_PG_DB="n8n"
HARBOR_N8N_ENCRYPTION_KEY="sk-harbor"
HARBOR_N8N_JWT_SECRET="sk-harbor"

# Bolt.new
HARBOR_BOLT_HOST_PORT=34201
HARBOR_BOLT_IMAGE="ghcr.io/stackblitz-labs/bolt.diy"
HARBOR_BOLT_VERSION="latest"

# Chat Nio
HARBOR_CHATNIO_HOST_PORT=34219
HARBOR_CHATNIO_MYSQL_HOST_PORT=34212
HARBOR_CHATNIO_REDIS_HOST_PORT=34213
HARBOR_CHATNIO_MYSQL_IMAGE="mysql"
HARBOR_CHATNIO_MYSQL_VERSION="latest"
HARBOR_CHATNIO_MYSQL_ROOT_PASSWORD=root
HARBOR_CHATNIO_MYSQL_DATABASE=chatnio
HARBOR_CHATNIO_MYSQL_USER=chatnio
HARBOR_CHATNIO_MYSQL_PASSWORD=chatnio123456!
HARBOR_CHATNIO_WORKPSACE="./chatnio"
HARBOR_CHATNIO_REDIS_IMAGE="redis"
HARBOR_CHATNIO_REDIS_VERSION="latest"
HARBOR_CHATNIO_IMAGE="programzmh/chatnio"
HARBOR_CHATNIO_VERSION="latest"

# Qdrant
HARBOR_QDRANT_HOST_PORT=34221
HARBOR_QDRANT_HOST_GRPC_PORT=34222
HARBOR_QDRANT_VERSION="latest"
HARBOR_QDRANT_API_KEY="4f81ba2b82664d63cc6af620f0cd1ba46e5894c2f4f148a94ec96c2c70ac5690"
HARBOR_QDRANT_READ_ONLY_API_KEY="ce103bf74f5de30d192b3b0151eeef95ba50ba99a87cc9c3ccf00ada033290db"
HARBOR_QDRANT_JWT_RBAC=true
HARBOR_QDRANT_DISABLE_TELEMETRY=true
HARBOR_QDRANT_LOG_LEVEL="INFO"

# K6
HARBOR_K6_HOST_PORT=34231
HARBOR_K6_OPEN_URL="http://localhost:34233"
HARBOR_K6_INFLUXDB_HOST_PORT=34232
HARBOR_K6_GRAFANA_HOST_PORT=34233
HARBOR_K6_IMAGE="grafana/k6"
HARBOR_K6_VERSION="latest"
HARBOR_K6_INFLUXDB_IMAGE="influxdb"
HARBOR_K6_INFLUXDB_VERSION="1.8"
HARBOR_K6_INFLUXDB_DB="k6"
HARBOR_K6_GRAFANA_IMAGE="grafana/grafana"
HARBOR_K6_GRAFANA_VERSION="10.2.4"

# Promptfoo
HARBOR_PROMPTFOO_HOST_PORT=34241
HARBOR_PROMPTFOO_IMAGE="ghcr.io/promptfoo/promptfoo"
HARBOR_PROMPTFOO_VERSION="latest"
HARBOR_PROMPTFOO_REMOTE_API_BASE_URL=http://promptfoo:3000
HARBOR_PROMPTFOO_REMOTE_APP_BASE_URL=http://localhost:34241
HARBOR_PROMPTFOO_WORKSPACE="./promptfoo/data"

# Webtop
HARBOR_WEBTOP_HOST_PORT=34251
HARBOR_WEBTOP_IMAGE="lscr.io/linuxserver/webtop"
HARBOR_WEBTOP_VERSION="ubuntu-kde"
HARBOR_WEBTOP_WORKSPACE="./webtop/data"

# Perplexideez
HARBOR_PERPLEXIDEEZ_HOST_PORT=34261
HARBOR_PERPLEXIDEEZ_IMAGE="ghcr.io/brunostjohn/perplexideez/app"
HARBOR_PERPLEXIDEEZ_VERSION="latest"
HARBOR_PERPLEXIDEEZ_MIGRATE_IMAGE="ghcr.io/brunostjohn/perplexideez/migrate:latest"

# Omniparser
HARBOR_OMNIPARSER_HOST_PORT=34271

# Flowise
HARBOR_FLOWISE_HOST_PORT=34281
HARBOR_FLOWISE_IMAGE="flowiseai/flowise"
HARBOR_FLOWISE_VERSION="latest"
HARBOR_FLOWISE_WORKSPACE="./flowise/data"

# LangFlow
HARBOR_LANGFLOW_IMAGE="langflowai/langflow"
HARBOR_LANGFLOW_VERSION=latest
HARBOR_LANGFLOW_HOST_PORT=34291
HARBOR_LANGFLOW_WORKSPACE="./langflow"

# OptiLLM
HARBOR_OPTILLM_HOST_PORT=34301
HARBOR_OPTILLM_WORKSPACE="./optillm/data"

# KoboldCpp
HARBOR_KOBOLD_HOST_PORT=34311
HARBOR_KOBOLD_IMAGE="koboldai/koboldcpp"
HARBOR_KOBOLD_VERSION="latest"
HARBOR_KOBOLD_WORKSPACE="./kobold/data"
HARBOR_KOBOLD_MODEL="https://huggingface.co/concedo/KobbleTinyV2-1.1B-GGUF/resolve/main/KobbleTiny-Q4_K.gguf?download=true"
HARBOR_KOBOLD_ARGS="--model model.gguf"

# Agent
HARBOR_AGENT_HOST_PORT=34321
HARBOR_AGENT_API_KEY="sk-agent"
HARBOR_AGENT_IMAGE="lscr.io/linuxserver/webtop"
HARBOR_AGENT_VERSION="ubuntu-xfce"
HARBOR_AGENT_WORKSPACE="./agent"

# Speaches
HARBOR_SPEACHES_HOST_PORT=34331
HARBOR_SPEACHES_VERSION="latest"
HARBOR_SPEACHES_STT_MODEL="Systran/faster-distil-whisper-large-v3"
HARBOR_SPEACHES_TTS_MODEL="hexgrad/Kokoro-82M"
HARBOR_SPEACHES_TTS_VOICE="af_bella"

# Morphic
HARBOR_MORPHIC_HOST_PORT=34341
HARBOR_MORPHIC_MODEL="llama3.1:8b"
HARBOR_MORPHIC_TOOL_MODEL="llama3.1:8b"

# SQLChat
HARBOR_SQLCHAT_HOST_PORT=34351
HARBOR_SQLCHAT_VERSION="latest"
HARBOR_SQLCHAT_IMAGE="sqlchat/sqlchat"
HARBOR_SQLCHAT_AUTH_SECRET="sk-harbor"

# GPTMe
HARBOR_GPTME_MODEL="llama3.1:8b"

# Mikupad
HARBOR_MIKUPAD_HOST_PORT=34361

# Traefik
HARBOR_TRAEFIK_HOST_PORT=80
HARBOR_TRAEFIK_HTTPS_HOST_PORT=443
HARBOR_TRAEFIK_METRICS_HOST_PORT=34372
HARBOR_TRAEFIK_DASHBOARD_HOST_PORT=34373
HARBOR_TRAEFIK_VERSION="v3"
HARBOR_TRAEFIK_IMAGE="traefik"
HARBOR_TRAEFIK_CERT_EMAIL="harbor@harbor.sh"
HARBOR_TRAEFIK_DOMAIN="lan"
HARBOR_TRAEFIK_CONFIG="./traefik/traefik.yml"
HARBOR_TRAEFIK_OPEN_URL="http://localhost:34373"

# Latent Scope
HARBOR_LATENTSCOPE_HOST_PORT=34381
HARBOR_LATENTSCOPE_WORKSPACE="./latentscope/data"

# Oterm
HARBOR_OTERM_WORKSPACE="./oterm/data"

# RAGLite
HARBOR_RAGLIT_HOST_PORT=34391
HARBOR_RAGLITE_WORKSPACE="./raglite/data"
HARBOR_RAGLITE_MODEL="ollama/llama3.1:8b"
HARBOR_RAGLITE_EMBEDDER="ollama/mxbai-embed-large:latest"

# llama-swap
HARBOR_LLAMASWAP_HOST_PORT=34401
HARBOR_LLAMASWAP_IMAGE="ghcr.io/mostlygeek/llama-swap"

# LibreTranslate
HARBOR_LIBRETRANSLATE_HOST_PORT=34411
HARBOR_LIBRETRANSLATE_VERSION="latest"
HARBOR_LIBRETRANSLATE_IMAGE="libretranslate/libretranslate"
HARBOR_LIBRETRANSLATE_WORKSPACE="./libretranslate/data"

# MetaMCP
HARBOR_METAMCP_HOST_PORT=34421
HARBOR_METAMCP_DB_HOST_PORT=34422
HARBOR_METAMCP_SSE_HOST_PORT=34423
HARBOR_METAMCP_GIT_REF="https://github.com/metatool-ai/metatool-app.git#v0.4.3"

# mcpo
HARBOR_MCPO_HOST_PORT=34431

# Local Deep Research
HARBOR_LDR_HOST_PORT=34441
HARBOR_LDR_WORKSPACE="./ldr/data"
HARBOR_LDR_IMAGE="localdeepresearch/local-deep-research"
HARBOR_LDR_VERSION="latest"

# LocalAI
HARBOR_LOCALAI_HOST_PORT=34451
HARBOR_LOCALAI_IMAGE="localai/localai"
HARBOR_LOCALAI_CPU_VERSION="latest-cpu"
HARBOR_LOCALAI_NVIDIA_VERSION="latest-gpu-nvidia-cuda-12"
HARBOR_LOCALAI_ROCM_VERSION="latest-gpu-hipblas"
HARBOR_LOCALAI_WORKSPACE="./localai/data"

# AgentZero
HARBOR_AGENTZERO_HOST_PORT=34461
HARBOR_AGENTZERO_VERSION="latest"
HARBOR_AGENTZERO_IMAGE="frdel/agent-zero-run"
HARBOR_AGENTZERO_WORKSPACE="./agentzero/data"

# Modular MAX
HARBOR_MODULARMAX_HOST_PORT=34471
HARBOR_MODULARMAX_IMAGE="docker.modular.com/modular/max-nvidia-full"
HARBOR_MODULARMAX_VERSION="latest"
HARBOR_MODULARMAX_MODEL="cognitivecomputations/Dolphin3.0-Qwen2.5-3b"
HARBOR_MODULARMAX_EXTRA_ARGS=""

# AirWeave
HARBOR_AIRWEAVE_HOST_PORT=34481
HARBOR_AIRWEAVE_BACKEND_HOST_PORT=34482
HARBOR_AIRWEAVE_POSTGRES_HOST_PORT=34483
HARBOR_AIRWEAVE_REDIS_HOST_PORT=34484
HARBOR_AIRWEAVE_QDRANT_HOST_PORT=34485
HARBOR_AIRWEAVE_EMBEDDINGS_HOST_PORT=34486
HARBOR_AIRWEAVE_GIT_REF="https://github.com/airweave-ai/airweave.git#main"
HARBOR_AIRWEAVE_WORKSPACE="./airweave/data"

# ============================================
# Service Configuration.
# You can specify any of the service's own environment variables here.
# ============================================

# General "do not track" settings affecting many services.
GA_DISABLE=1
DO_NOT_TRACK=1
GOOGLE_ANALYTICS_DISABLE=1
GRADIO_ANALYTICS_ENABLED=false

# Open WebUI
# See https://docs.openwebui.com/getting-started/env-configuration/ for reference.
# --------------------------------------------
# WEBUI_NAME=WUI

# Ollama Configuration.
# Run harbor ollama serve --help for a list of env vars
# --------------------------------------------
# OLLAMA_DEBUG=1
# OLLAMA_NUM_PARALLEL=4

# vLLM Configuration
# See https://docs.vllm.ai/en/latest/serving/env_vars.html for reference
# --------------------------------------------
VLLM_NO_USAGE_STATS=true
VLLM_DO_NOT_TRACK=1
